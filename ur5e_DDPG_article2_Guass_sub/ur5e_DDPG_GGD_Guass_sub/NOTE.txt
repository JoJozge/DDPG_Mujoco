训练时间：2023.08.15/20:14------2023.08.16/13:34训练至20000回合,已经达到预期效果。
目的：生成不同的参数，检测不同的参数在TEST中的影响。
1.0版本训练总结:
已经训练到17896回合，本次训练到现在并未取到很好的奖励值，并且获得奖励值的稳定性十分差，现分析有以下几点因素造成：
1.reward_engineering:奖励值设置不合理，因为之前目标物体比较大，而本次实验将获得+10奖励值的范围缩小到了半径为0.01m的半径球体空间范围内。导致智能体在探索的过程中无法触碰到该物体从而的到奖励，所以需要重新设计一下奖励值的组成结构。
2.replay_buffer:数据缓存区设置值为1E7，相较于100的batch_size,感觉前者设置值过大，有一点影响程序运行效率，同时两者比例相差过大反而对神经网络更新效果不好，下次训练会采取更小的数据缓存区。
3.update_iteration:神经网络更新间隔，设置值为500，感觉相比于100的batch_size两者相差也过大，需要进行优化。
1.1版本训练总结:
内容调整:修改了奖励值的判定范围，加大了给予奖励的范围。修改了数据缓存区，现在为1E6，batch_size为300。神经网络更新间隔未修改。
1.经过检查发现DDPG中的update_iteration参数有可能可以去掉，因为加入这个超参数意味着在一个episode会进行500次（当设置的值为500时）神经网络参数更新，在DDPG中，target网络采用的是软更新策略。
1.2版本训练总结:
1.经过检查发现DDPG中的update_iteration参数有可能可以去掉，因为加入这个超参数意味着在一个episode会进行500次（当设置的值为500时）神经网络参数更新，在DDPG中，target网络采用的是软更新策略,1.2版本是由于1.1版本意外中断而开始训练的，本质上是1.1版本的重新训练，没有进行过多的修改，只是对奖励模型进行了微调。
2.在改进reward_engineering之后现在认为可能需要进一步改进。
3.神经网络层数需要修改。
4.因为失误而修改了影响智能体探索能力的语句。
1.3版本训练总结:
1.经过检查发现DDPG中的update_iteration参数有可能可以去掉，因为加入这个超参数意味着在一个episode会进行500次（当设置的值为500时）神经网络参数更新，在DDPG中，target网络采用的是软更新策略,1.2版本是由于1.1版本意外中断而开始训练的，本质上是1.1版本的重新训练，没有进行过多的修改，只是对奖励模型进行了微调。
2.在改进reward_engineering之后现在认为可能需要进一步改进。
3.神经网络层数需要修改。
4.因为失误而修改了影响智能体探索能力的语句，现在已经更改过来了。
1.4版本训练总结:
1.已经基于之前的成功训练的经验修改了部分代码和参数，现在再进行尝试一下。
2. 1.4.0版本的代码，奖励范围设定值过小导致训练效果一直不好。
3. 1.4.1版本代码应该是探索值过大导致不太稳定现在尝试在各自的基础上进行调整之后再做实验。
4.两次训练均未记录时间，现重新开始记录训练时间。
1.4.1.1版本训练总结:
1.基于1.4.1版本修改了影响探索能力的系数。

1.4.1.2版本训练总结:
1.基于1.4.1.1版本修改了影响target网络更新的系数-update_iteration。
2.在减小了-update_iteration果然更稳定了，但相对而言也更加难以找到最优的reward了，目前增大了该值至200，开始1.4.1.3训练。

1.4.1.3版本训练总结:
1.训练到1000回合左右开始出现反复震荡，此时并没有达到很好的效果，目前推断原因如下。探索系数过小，神经网络神经元个数不够。仍在继续观察中。
2.探索系数为0.05，-update_iteration为200，虽然可以达到目标点，但是波动过大。

1.4.1.4版本训练总结:
1 1.4.1.4版本与1.4.1.5本来是对照组，但是为何只是修改了探索参数，相差就如此之大？而且按道理，探索系数越大，应该更加不稳定才对，可以目前效果来看反而更好。

1.4.1.5版本训练总结:
1.探索参数为0.01，-update_iteration为100的情况下已经可以得出比较理想的结果。

1.4.1.6版本训练总结:是对1.4.1.5版本的第二次训练，任何参数都没有修改。
1. 实验证明了目前的参数是比较合适的，不需要进行修改。

1.4.1.7版本训练总结:将随机数初始化种子设置为定值，此后的随机数的种子为一定的。
1. 修改目标实体模型为视觉模型（无碰撞体积），修改之后实验效果明显更好。

1.4.1.8版本训练总结：在1.4.1.7版本的基础上减小了目标点
1. 在缩小目标点和奖励后，该参数仍然可以很好地完成任务。

1.4.1.8.1版本训练总结：在1.4.1.8版本的基础上加上了生成运动轨迹的代码。
1.可以很好地输出相应的轨迹。

GGD_0.1版本训练总结：在1.4.1.8.1版本上进行论文实验的代码，本质上没什么区别，只是根据论文需求修改输出的图。
1.正态分布噪声标准差为0.1。

GGD_0.1_one版本训练总结：在GGD_0.1版本上进行论文实验的代码，本质上没什么区别，只是根据论文需求修改输出的图。
1.未修改奖励值随episode变化的图。
2.修改了某个episode里面关节角度随step变化的图。
3.修改了轨迹的输出，现在可以同步输出。

GGD_0.1_two版本训练总结：在GGD_0.1_one版本上进行论文实验的代码，本质上没什么区别，只是修改了数据保存的间隔。

GGD_0.1_three版本训练总结：在GGD_0.1_two版本上进行论文实验的代码，新增了几个文件夹。

GGD_0.1_four版本训练总结：在GGD_0.1_three版本上修改了出图的格式。

注意：five即以上版本采用了新的action_bound范围，原因是原来的action_bound范围是错误的。
GGD_0.1_five版本训练总结：在GGD_0.1_four版本上，修改了action_bound的取值范围。

GGD_0.002_six版本训练总结：
1.对action_bound进行了修改。
2.现在可以选择生成CSV或直接生成plt。
3.带有csv后缀的文件夹保存csv格式的表格，再利用对应的csv2png项目转化成png格式的图片。
4.end_motion_path不能生成图片。

GGD_template_six版本训练总结：
1.优化了出图方法。
2.设计了多个文件夹以保存csv数据。
3.可以在训练过程中不画图，把分析工作与绘图工作分开。
4.与ur5e_DDPG_article1文件夹中的six版本相差不大。ur5e_DDPG_article1文件夹中训练过程是为了完善该版本。





